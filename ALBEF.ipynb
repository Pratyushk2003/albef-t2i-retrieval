{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP025gBygHVFm50EN/InnC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pratyushk2003/albef-t2i-retrieval/blob/main/ALBEF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJt_FlHYgsZd",
        "outputId": "47f94e78-7d49-4825-c186-32c84811748a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ALBEF'...\n",
            "remote: Enumerating objects: 353, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 353 (delta 87), reused 82 (delta 82), pack-reused 208\u001b[K\n",
            "Receiving objects: 100% (353/353), 71.56 MiB | 35.01 MiB/s, done.\n",
            "Resolving deltas: 100% (134/134), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/salesforce/ALBEF.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eGzO38plyBu",
        "outputId": "5cb33a24-9557-4e58-e546-09bbe014c3ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/ALBEF/')"
      ],
      "metadata": {
        "id": "MxL7bGlpguap"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "xn0ARz3lhJ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.8.1"
      ],
      "metadata": {
        "id": "CPAShRegiYcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "file_path = '/kaggle/working/ALBEF/models/xbert.py'\n",
        "with open(file_path, 'r') as file:\n",
        "    file_content = file.read()\n",
        "modified_content = re.sub(r'\\btokenizer_class\\b', 'processor_class', file_content)\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(modified_content)"
      ],
      "metadata": {
        "id": "5pk601oLVepV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from models.vit import VisionTransformer\n",
        "from models.xbert import BertConfig, BertModel\n",
        "from models.tokenization_bert import BertTokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "import json\n",
        "\n",
        "class VL_Transformer_ITM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 text_encoder = None,\n",
        "                 config_bert = ''\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        bert_config = BertConfig.from_json_file(config_bert)\n",
        "        self.visual_encoder = VisionTransformer(\n",
        "            img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
        "            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "        self.text_encoder = BertModel.from_pretrained(text_encoder, config=bert_config, add_pooling_layer=False)\n",
        "\n",
        "        self.itm_head = nn.Linear(768, 2)\n",
        "\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        image_embeds = self.visual_encoder(image)\n",
        "        print(\"image embeddings\",image_embeds)\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
        "\n",
        "        output = self.text_encoder(text.input_ids,\n",
        "                                attention_mask = text.attention_mask,\n",
        "                                encoder_hidden_states = image_embeds,\n",
        "                                encoder_attention_mask = image_atts,\n",
        "                                return_dict = True,\n",
        "                               )\n",
        "\n",
        "        vl_embeddings = output.last_hidden_state[:,0,:]\n",
        "        vl_output = self.itm_head(vl_embeddings)\n",
        "        return output, image_embeds"
      ],
      "metadata": {
        "id": "hNoT3uQsi2V4"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, VisionEncoderDecoderModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "CjuNvqKDi7kf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VL_Transformer_ITM(text_encoder='bert-base-uncased', config_bert='/content/ALBEF/configs/config_bert.json')"
      ],
      "metadata": {
        "id": "kddpTMEhgwez"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def pre_caption(caption,max_words=30):\n",
        "    caption = re.sub(\n",
        "        r\"([,.'!?\\\"()*#:;~])\",\n",
        "        '',\n",
        "        caption.lower(),\n",
        "    ).replace('-', ' ').replace('/', ' ')\n",
        "\n",
        "    caption = re.sub(\n",
        "        r\"\\s{2,}\",\n",
        "        ' ',\n",
        "        caption,\n",
        "    )\n",
        "    caption = caption.rstrip('\\n')\n",
        "    caption = caption.strip(' ')\n",
        "\n",
        "    #truncate caption\n",
        "    caption_words = caption.split(' ')\n",
        "    if len(caption_words)>max_words:\n",
        "        caption = ' '.join(caption_words[:max_words])\n",
        "    return caption"
      ],
      "metadata": {
        "id": "kiy7ZfVUm8U7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from skimage import transform as skimage_transform\n",
        "from scipy.ndimage import filters\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def getAttMap(img, attMap, blur = True, overlap = True):\n",
        "    attMap -= attMap.min()\n",
        "    if attMap.max() > 0:\n",
        "        attMap /= attMap.max()\n",
        "    attMap = skimage_transform.resize(attMap, (img.shape[:2]), order = 3, mode = 'constant')\n",
        "    if blur:\n",
        "        attMap = filters.gaussian_filter(attMap, 0.02*max(img.shape[:2]))\n",
        "        attMap -= attMap.min()\n",
        "        attMap /= attMap.max()\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    attMapV = cmap(attMap)\n",
        "    attMapV = np.delete(attMapV, 3, 2)\n",
        "    if overlap:\n",
        "        attMap = 1*(1-attMap**0.7).reshape(attMap.shape + (1,))*img + (attMap**0.7).reshape(attMap.shape+(1,)) * attMapV\n",
        "    return attMap\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384,384),interpolation=Image.BICUBIC),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])"
      ],
      "metadata": {
        "id": "chVSXqWlm-ew"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "images = Image.open(requests.get(url, stream=True).raw)\n",
        "text_input = \"a photo of a cat\"\n"
      ],
      "metadata": {
        "id": "RBsYzvVtgyqc"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "image = transform(images).unsqueeze(0)\n",
        "caption = 'a photo of a cat'\n",
        "text = pre_caption(caption)\n",
        "text_input = tokenizer(text, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "PhUOuMYZkrcC"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output, image_embed = model(image, text_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY_HpAL-kvv5",
        "outputId": "64d44a55-3fe9-42c2-c2c2-bde5caa95cf6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image embeddings tensor([[[ 0.4824,  0.9153, -1.9881,  ..., -0.4806,  0.0265, -0.2992],\n",
            "         [ 1.0946, -0.0651, -2.6840,  ...,  0.1419, -0.5108, -0.9272],\n",
            "         [ 1.3450,  0.0102, -2.8977,  ..., -0.1485, -0.8198, -1.2747],\n",
            "         ...,\n",
            "         [ 1.5173,  0.5864, -3.2647,  ..., -1.3810, -1.1761, -2.1298],\n",
            "         [ 1.1281,  0.3195, -3.2699,  ..., -1.4792, -1.2343, -1.9161],\n",
            "         [ 1.5978,  0.6517, -3.0029,  ..., -1.6326, -1.1732, -2.0038]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings  = output['last_hidden_state']"
      ],
      "metadata": {
        "id": "EScbBwykoAWJ"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_embed.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7Xt0TU2sLhp",
        "outputId": "d136be56-e419-469b-e788-4819d295b720"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 577, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhKyqcPdrzWP",
        "outputId": "44249eee-ef0e-400b-e95b-22993ecb4a67"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_scores = cosine_similarity(image_embed[0,0].detach().numpy().reshape(1, -1), text_embeddings[0,0].detach().numpy().reshape(1, -1))"
      ],
      "metadata": {
        "id": "pxBPURt5vPn0"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCoBo9azvrk7",
        "outputId": "8c205420-83ca-47e3-e2d9-1e445bb55f0a"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02134769]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    }
  ]
}